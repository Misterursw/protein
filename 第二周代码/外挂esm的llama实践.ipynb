{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库和模块\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer  # 用于加载预训练模型，例如ESM-1b，ESM-IF1，Vicuna-13b\n",
    "\n",
    "### Step 1: 数据集的加载与预处理\n",
    "class ProteinDataset:\n",
    "    def __init__(self, seq_file, structure_file, description_file):\n",
    "        # 载入蛋白质1D序列、3D结构和文本描述数据\n",
    "        self.sequences = load_sequences(seq_file)  # 加载蛋白质1D序列\n",
    "        self.structures = load_structures(structure_file)  # 加载蛋白质3D结构\n",
    "        self.descriptions = load_descriptions(description_file)  # 加载对应的文本描述\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.structures[idx], self.descriptions[idx] \n",
    "\n",
    "# 定义数据加载器\n",
    "train_dataset = ProteinDataset('train_seq.txt', 'train_structure.pdb', 'train_description.txt')\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "### Step 2: 初始化预训练模型 (ESM-1b, ESM-IF1, Vicuna-13b)\n",
    "\n",
    "# 加载预训练的蛋白质编码器\n",
    "seq_encoder = AutoModel.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")  # 1D序列编码器 ESM-1b\n",
    "structure_encoder = AutoModel.from_pretrained(\"facebook/esm_if1_gvp4_t16\")  # 3D结构编码器 ESM-IF1\n",
    "\n",
    "# 加载预训练的语言模型 (Vicuna-13b)\n",
    "language_model = AutoModel.from_pretrained(\"lmsys/vicuna-13b\")  # Vicuna-13b LLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-13b\")\n",
    "\n",
    "### Step 3: 定义 PLP-former 模块\n",
    "class PLPFormer(nn.Module):\n",
    "    def __init__(self, seq_dim, structure_dim, hidden_dim):\n",
    "        super(PLPFormer, self).__init__()\n",
    "        # Transformer 模块用于对蛋白质嵌入进行处理\n",
    "        self.transformer = nn.Transformer(d_model=hidden_dim, nhead=8, num_encoder_layers=4)\n",
    "        self.linear_seq = nn.Linear(seq_dim, hidden_dim)  # 将1D嵌入投射到隐藏维度\n",
    "        self.linear_structure = nn.Linear(structure_dim, hidden_dim)  # 将3D嵌入投射到隐藏维度\n",
    "\n",
    "    def forward(self, seq_embedding, structure_embedding):\n",
    "        # 将1D和3D嵌入映射到统一的空间\n",
    "        seq_proj = self.linear_seq(seq_embedding)\n",
    "        structure_proj = self.linear_structure(structure_embedding)\n",
    "        \n",
    "        # 合并1D和3D信息，输入到Transformer中\n",
    "        combined_embedding = torch.cat((seq_proj, structure_proj), dim=1)\n",
    "        output_embedding = self.transformer(combined_embedding)\n",
    "        return output_embedding\n",
    "\n",
    "# 初始化 PLP-former\n",
    "plp_former = PLPFormer(seq_dim=1280, structure_dim=512, hidden_dim=768)\n",
    "\n",
    "### Step 4: 定义投影适配器\n",
    "class ProjectionAdapter(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionAdapter, self).__init__()\n",
    "        self.adapter = nn.Linear(input_dim, output_dim)  # 将PLP-former的输出映射到Vicuna-13b的输入维度\n",
    "\n",
    "    def forward(self, plp_output):\n",
    "        return self.adapter(plp_output)\n",
    "\n",
    "# 初始化投影适配器\n",
    "projection_adapter = ProjectionAdapter(input_dim=768, output_dim=4096)  # 假设Vicuna-13b的输入维度为4096\n",
    "\n",
    "### Step 5: 定义完整的 ProtChatGPT 模型\n",
    "class ProtChatGPT(nn.Module):\n",
    "    def __init__(self, seq_encoder, structure_encoder, plp_former, projection_adapter, language_model):\n",
    "        super(ProtChatGPT, self).__init__()\n",
    "        self.seq_encoder = seq_encoder  # 1D序列编码器\n",
    "        self.structure_encoder = structure_encoder  # 3D结构编码器\n",
    "        self.plp_former = plp_former  # PLP-former\n",
    "        self.projection_adapter = projection_adapter  # 投影适配器\n",
    "        self.language_model = language_model  # Vicuna-13b\n",
    "\n",
    "    def forward(self, seq_input, structure_input, question):\n",
    "        # 1. 编码蛋白质1D序列和3D结构\n",
    "        seq_embedding = self.seq_encoder(seq_input).last_hidden_state\n",
    "        structure_embedding = self.structure_encoder(structure_input).last_hidden_state\n",
    "        \n",
    "        # 2. 使用 PLP-former 对齐蛋白质信息\n",
    "        plp_output = self.plp_former(seq_embedding, structure_embedding)\n",
    "        \n",
    "        # 3. 使用投影适配器将输出嵌入投影到语言模型的输入空间\n",
    "        adapted_embedding = self.projection_adapter(plp_output)\n",
    "        \n",
    "        # 4. 使用语言模型生成回答\n",
    "        question_tokens = tokenizer(question, return_tensors='pt').input_ids\n",
    "        lm_output = self.language_model(inputs_embeds=adapted_embedding, labels=question_tokens)\n",
    "        return lm_output\n",
    "\n",
    "# 初始化 ProtChatGPT 模型\n",
    "protchatgpt = ProtChatGPT(seq_encoder, structure_encoder, plp_former, projection_adapter, language_model)\n",
    "\n",
    "### Step 6: 训练过程\n",
    "optimizer = torch.optim.Adam(protchatgpt.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for seq_input, structure_input, description in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = protchatgpt(seq_input, structure_input, description)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(output.logits.view(-1, output.logits.size(-1)), description.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"模型训练完毕\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 数据集类型 | 读取样本数量 | 读取用时 (s) | CPU 使用率 (%) | 内存使用 (MB) | 数据处理样本数量 | 平均处理时间 (s) | 总处理用时 (s) |\n",
    "|------------|--------------|--------------|----------------|--------------|------------------|-----------------|----------------|\n",
    "| KITTI      | 5000         | 4.4650       | 0.80           | 23.88        | 5000             | 0.089098         | 445.4990       |\n",
    "| MindRecord | 5000         | 0.1638       | 2.10           | 11.21        | 5000             | 0.039375         | 196.8923       |\n",
    "|------------|--------------|--------------|----------------|--------------|------------------|-----------------|----------------|\n",
    "| Libritts      | 5000         | 2.7889       | 1.20           | 45.55        | 500              | 0.014486         | 7.2438         |\n",
    "| MindRecord | 5000         | 2.4033       | 0.30           | 8.06         | 500              | 0.008484         | 4.2429         |\n",
    "|------------|--------------|--------------|----------------|--------------|------------------|-----------------|----------------|\n",
    "| Ljspeech    | 50000        | 2.6579       | 0.50           | 49.95        | 5000             | 0.008219         | 41.1026        |\n",
    "| MindRecord  | 50000        | 2.4380       | 0.20           | 3.86         | 5000             | 0.009397         | 46.9934        |\n",
    "|------------|--------------|--------------|----------------|--------------|------------------|-----------------|----------------|\n",
    "| Squad      | 5000         | 9.0251       | 0.00           | 938.77       | 50               | 6.166194        | 308.3099       |\n",
    "| MindRecord  | 5000         | 2.8578       | 0.00           | 1.63         | 50               | 0.464135        | 23.2070        |\n",
    "|------------|--------------|--------------|----------------|--------------|------------------|-----------------|----------------|\n",
    "| SST        | 500          | 0.2428       | 0.60           | 10.21        | 500              | 0.214583        | 107.2924       |\n",
    "| MindRecord | 500          | 0.2616       | 1.60           | 38.04        | 500              | 0.235432        | 117.7181       |\n",
    "|------------|--------------|--------------|----------------|--------------|------------------|-----------------|----------------|\n",
    "| MindRecord | 100          | 0.3102       | 0.60           | 61.35        | 100              | 0.210007        | 21.0011        |\n",
    "| VOC        | 100          | 0.4328       | 3.10           | 8.24         | 100              | 0.373909        | 37.3911        |\n",
    "|------------|--------------|--------------|----------------|--------------|------------------|-----------------|----------------|\n",
    "| Wiki       | 50000        | 1.4066       | 0.80           | 13.94        | 500              | 0.052930         | 26.4657        |\n",
    "| MindRecord | 50000        | 4.7785       | 1.70           | 29.96        | 500              | 0.152549        | 76.2847        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
